{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43c19b29-f106-4111-ae53-f4c728a3ecb1"
   },
   "source": [
    "# Evaluating Answer Quality and Retrieval Quality of Agentic RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39dc54d3-5342-4cd9-8379-f3983dbbdf50"
   },
   "source": [
    "This notebook demonstrates the following:\n",
    "\n",
    "* Create an Agent with two tools.\n",
    "* One tool is, for a given question, retrieve the context from EU AI Act summary document from https://artificialintelligenceact.eu/high-level-summary/\n",
    "* Another tool is, for a given question, retrieve the context from watsonx FAQs.\n",
    "* Against the Agent, run couple of questions related to EU AI Act and watsonx FAQs using the system prompt and human prompt as described in the notebook.\n",
    "* As part of this process, collect the following details - question, context from either EU AI Act related tool or from FAQ related tool, and the respective answers.\n",
    "* Then, create a Detached Prompt Template using the combination of the system prompt + human prompt.\n",
    "* Use Mistral model as the RAG LLM-as-a-Judge evaluator for evaluating RAG metrics.\n",
    "* Log and evaluate the metrics.\n",
    "* And visualise the metrics via., watsonx.governance UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1afb655f-03dd-4da1-a4c9-ccbfa542a645"
   },
   "source": [
    "## Setup <a name=\"settingup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1075d4fc-cf23-4c75-b6cf-c97080b52846"
   },
   "outputs": [],
   "source": [
    "!pip install -U ibm-watson-openscale | tail -n 1\n",
    "!pip install --upgrade ibm-watsonx-ai | tail -n 1\n",
    "!pip install langchain | tail -n 1\n",
    "!pip install langchain-ibm | tail -n 1\n",
    "!pip install langchain-community | tail -n 1\n",
    "!pip install ibm_watson_machine_learning | tail -n 1\n",
    "!pip install chromadb | tail -n 1\n",
    "!pip install tiktoken | tail -n 1\n",
    "!pip install --upgrade ibm-aigov-facts-client | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4012b1e-b406-4f2b-abb9-8839a17e3e02"
   },
   "source": [
    "### Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7d29ffd6-d07b-4eec-a975-2e28419af5ae"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b22fc40-45d6-4557-8039-6e8536c8dfb8"
   },
   "source": [
    "## Imports <a name=\"Necessary Imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ffdf8565-db6f-4492-9cd0-55e40723b79f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxEmbeddings, WatsonxLLM\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1c1166f-6bcb-4009-aaf0-89e12e236add"
   },
   "source": [
    "### Configure your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d17f26b6-ccca-4714-8576-2308f23bf9a3"
   },
   "outputs": [],
   "source": [
    "IAM_URL = \"https://iam.cloud.ibm.com\"\n",
    "DATAPLATFORM_URL = \"https://api.dataplatform.cloud.ibm.com\"\n",
    "FACTSHEET_URL = \"https://dataplatform.cloud.ibm.com\"\n",
    "SERVICE_URL = \"https://aiopenscale.cloud.ibm.com\"\n",
    "CLOUD_API_KEY = \"xxxxxxxx\"\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": CLOUD_API_KEY,\n",
    "}\n",
    "CREDENTIALS = credentials\n",
    "project_id = \"f1774d00-d497-459c-xxxx-xxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebe098e3-5f3d-424d-8a4f-f4d95827280f"
   },
   "source": [
    "## watsonx LLM for Prompt Generation <a name=\"Prompt Generation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6d329d81-442d-47cb-9a65-5bed5a44132e"
   },
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-3-8b-instruct\", \n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params={\n",
    "        GenParams.DECODING_METHOD: \"greedy\",\n",
    "        GenParams.TEMPERATURE: 0,\n",
    "        GenParams.MIN_NEW_TOKENS: 5,\n",
    "        GenParams.MAX_NEW_TOKENS: 250,\n",
    "        GenParams.STOP_SEQUENCES: [\"Human:\", \"Observation\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f48349a8-ff7e-4120-938d-c36e66f6dacb"
   },
   "source": [
    "## Slate Model for Embeddings Generation <a name=\"Embeddings Generation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "51f72c5a-7d99-4a0e-9195-582b778772fc"
   },
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    embeddings = WatsonxEmbeddings(\n",
    "        model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "        url=credentials[\"url\"],\n",
    "        apikey=credentials[\"apikey\"],\n",
    "        project_id=project_id,\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c6dec697-f979-46d5-a431-ea935473304f"
   },
   "outputs": [],
   "source": [
    "def get_text_splitter():\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "    return text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63aa308a-a92a-4721-bd2d-2738c95561b3"
   },
   "source": [
    "## Doc URLS <a name=\"Doc URLS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "29931b66-efcc-4fa4-8830-9e1e39c978e8"
   },
   "outputs": [],
   "source": [
    "gov_faq_urls = [\n",
    "    'https://www.ibm.com/docs/en/watsonx/saas?topic=overview-faq'\n",
    "]\n",
    "\n",
    "ai_act_urls = [\n",
    "    'https://artificialintelligenceact.eu/high-level-summary/'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed0576d0-090c-423c-ae54-58094171ea4b"
   },
   "source": [
    "## Vector Store Retriever against a given doc source<a name=\"Vector Store Retriever\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1ca5cc5f-f338-4518-8533-484e0281c1f6"
   },
   "outputs": [],
   "source": [
    "def get_retriever(urls, collection_name):\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "    text_splitter = get_text_splitter()\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=doc_splits,\n",
    "        collection_name=collection_name,\n",
    "        embedding=get_embeddings(),\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c92c511-5573-4572-9f5c-bf30d249b8d3"
   },
   "source": [
    "## Retriever for FAQs document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0ec06b60-09c0-494e-ac92-d4bacfb61561"
   },
   "outputs": [],
   "source": [
    "gov_faqs_retriever = get_retriever(urls=gov_faq_urls, collection_name='gov_faqs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b834624-c5fa-43f2-8f95-4f6e4b0cddc7"
   },
   "source": [
    "## Retriever for EU AI Act document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2f62111f-4c44-49bb-9695-fb1d44ca7311"
   },
   "outputs": [],
   "source": [
    "ai_act_retriever = get_retriever(urls=ai_act_urls, collection_name='ai_act')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b2db67a-de70-4752-8b2c-ae3ba68d115a"
   },
   "source": [
    "## Agentic AI tools. One wrapping the FAQs and other wrapping the EU AI Act summary document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "488f9484-1dfe-49a3-820c-30fc0a3b2565"
   },
   "outputs": [],
   "source": [
    "ai_act_context = \"\"\n",
    "gov_faqs_context = \"\"\n",
    "\n",
    "@tool\n",
    "def get_Gov_FAQs_Context(question: str):\n",
    "    \"\"\"Get context from watsonx.ai, watsonx.governance, OpenScale related Frequently asked questions.\"\"\"\n",
    "    global gov_faqs_context\n",
    "    gov_faqs_context = gov_faqs_retriever.invoke(question)\n",
    "    return gov_faqs_context\n",
    "\n",
    "@tool\n",
    "def get_AI_Act_Summary_Context(question: str):\n",
    "    \"\"\"Get context from High-level summary of the AI Act.\"\"\"\n",
    "    global ai_act_context\n",
    "    ai_act_context = ai_act_retriever.invoke(question)\n",
    "    return ai_act_context\n",
    "\n",
    "tools = [get_AI_Act_Summary_Context, get_Gov_FAQs_Context]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb342e57-55f5-41d8-83dd-92fb78d8b4ef"
   },
   "source": [
    "## Standard Langchain based System Prompt working as an Generative AI Agent\n",
    "\n",
    "Next, we will set up a new prompt template to ask multiple questions. This template is more complex. It is referred to as a [structured chat prompt](https://api.python.langchain.com/en/latest/agents/langchain.agents.structured_chat.base.create_structured_chat_agent.html#langchain-agents-structured-chat-base-create-structured-chat-agent) and can be used for creating agents that have multiple tools available. In our case, the tool we are using was defined in Step 6. The structured chat prompt will be made up of a `system_prompt`, a `human_prompt` and our RAG tool. \n",
    "\n",
    "First, we will set up the `system_prompt`. This prompt instructs the agent to print its \"thought process,\" which involves the agent's subtasks, the tools that were used and the final output. This gives us insight into the agent's function calling. The prompt also instructs the agent to return its responses in JSON Blob format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ec330646-2c8c-424d-9e2c-6eb5a0308cf8"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\"\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Follow this format:\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
    "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce799d3-146f-457f-8542-9af7d54bd768"
   },
   "source": [
    "## The human prompt\n",
    "\n",
    "In the following code, we are establishing the `human_prompt`. This prompt tells the agent to display the user input followed by the intermediate steps taken by the agent as part of the `agent_scratchpad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7c29eed5-219b-4b0a-aead-787da37d1cd5"
   },
   "outputs": [],
   "source": [
    "human_prompt = \"\"\"{input}\n",
    "{agent_scratchpad}\n",
    "(reminder to always respond in a JSON blob)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d023856-69b4-40f2-8970-313fc846037f"
   },
   "source": [
    "Next, we establish the order of our newly defined prompts in the prompt template. We create this new template to feature the `system_prompt` followed by an optional list of messages collected in the agent's memory, if any, and finally, the `human_prompt` which includes both the human input and `agent_scratchpad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1ddd3aa3-df2b-4a7b-9ee3-22b308628373"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "267a90df-211b-48a0-a77d-7b87bcbbc8bb"
   },
   "source": [
    "Now, let's finalize our prompt template by adding the tool names, descriptions and arguments using a [partial prompt template](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/partial/). This allows the agent to access the information pertaining to each tool including the intended use cases and also means we can add and remove tools without altering our entire prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "507e24be-78a9-46c3-b4c3-0ece533c36fb"
   },
   "outputs": [],
   "source": [
    "prompt = prompt.partial(\n",
    "    tools=render_text_description_and_args(list(tools)),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ad9f708-7de3-4d1a-b7d1-217d0a072b2e"
   },
   "source": [
    "## Set up the agent's memory and chain\n",
    "\n",
    "An important feature of AI agents is their memory. Agents are able to store past conversations and past findings in their memory to improve the accuracy and relevance of their responses going forward. In our case, we will use LangChain's `ConversationBufferMemory()` as a means of memory storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "13b1eae8-f20c-4cd6-9d2c-83ff8d540dd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wsuser/ipykernel_783/995385594.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "b3fa682a-a44f-461c-8e41-8a47d937c3ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\n",
      "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
      "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
      "Provide only ONE action per $JSON_BLOB, as shown:\"\n",
      "```\n",
      "{{\n",
      "  \"action\": $TOOL_NAME,\n",
      "  \"action_input\": $INPUT\n",
      "}}\n",
      "```\n",
      "Follow this format:\n",
      "Question: input question to answer\n",
      "Thought: consider previous and subsequent steps\n",
      "Action:\n",
      "```\n",
      "$JSON_BLOB\n",
      "```\n",
      "Observation: action result\n",
      "... (repeat Thought/Action/Observation N times)\n",
      "Thought: I know what to respond\n",
      "Action:\n",
      "```\n",
      "{{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Final response to human\"\n",
      "}}\n",
      "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
      "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\n",
      "\n",
      "{input}\n",
      "{agent_scratchpad}\n",
      "(reminder to always respond in a JSON blob)\n"
     ]
    }
   ],
   "source": [
    "prompt_input = prompt.messages[0].prompt.template + '\\n\\n' + prompt.messages[2].prompt.template\n",
    "print(prompt_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03fb4493-8a41-4b85-8d45-c4f5a369ebd2"
   },
   "source": [
    "And now we can set up a chain with our agent's scratchpad, memory, prompt and the LLM. The AgentExecutor class is used to execute the agent. It takes the agent, its tools, error handling approach, verbose parameter and memory as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "943aa7cb-cc68-4424-ba3e-68257cd43a44"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        chat_history=lambda x: memory.chat_memory.messages,\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=chain, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07acd16c-b406-437e-af8d-48746d75585e"
   },
   "source": [
    "## Utility method to construct the context as retrieved from respective vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "03be8021-6e1a-4cda-8ddb-7db7c4266766"
   },
   "outputs": [],
   "source": [
    "def construct_context(relevant_context):\n",
    "    context_str = ''\n",
    "    for doc in relevant_context:\n",
    "        context_str = context_str + doc.page_content + '\\n\\n'\n",
    "    return context_str    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "3f832cbc-a365-4f31-8461-7ccbb640edf9"
   },
   "outputs": [],
   "source": [
    "# Store the complete the context\n",
    "complete_content = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a919b009-2a7c-4bd5-96cc-f2ec8009073d"
   },
   "source": [
    "## Generate responses with the agentic RAG system\n",
    "\n",
    "We are now able to ask the agent questions. Recall the agent's previous inability to provide us with information pertaining to the 2024 US Open. Now that the agent has its RAG tool available to use, let's try asking the same questions again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "908321ac-7b7c-4a0e-8f10-ffa73d06cb53"
   },
   "source": [
    "### Question 1. Related to FAQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "438803ec-5071-40cd-9763-80c0cd0add11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Question: How is model bias mitigated by using Watson OpenScale?\n",
      "Thought: The user is asking about the role of Watson OpenScale in mitigating model bias. I should use the get_Gov_FAQs_Context tool to find the relevant information.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_Gov_FAQs_Context\",\n",
      "  \"action_input\": \"How does Watson OpenScale mitigate model bias?\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[33;1m\u001b[1;3m[Document(metadata={'description': 'Find answers to frequently asked questions about watsonx.ai.', 'language': 'en', 'source': 'https://www.ibm.com/docs/en/watsonx/saas?topic=overview-faq', 'title': 'Frequently asked questions'}, page_content='What does it mean if the fairness score is greater than 100 percent?\\nDepending on your fairness configuration, your fairness score can exceed 100 percent. It means that your monitored group is getting relatively more “fair” outcomes as compared to the reference group. Technically, it means that the model is\\n              unfair in the opposite direction.\\n\\n\\nHow is model bias mitigated by using Watson OpenScale?\\nThe debiasing capability in Watson OpenScale is enterprise grade. It is robust, scalable and can handle a wide variety of models. Debiasing in Watson OpenScale consists of a two-step process: Learning Phase: Learning customer model behavior\\n              to understand when it acts in a biased manner.\\nApplication Phase: Identifying whether the customer’s model acts in a biased manner on a specific data point and, if needed, fixing the bias. For more information, see Debiasing options.'), Document(metadata={'description': 'Find answers to frequently asked questions about watsonx.ai.', 'language': 'en', 'source': 'https://www.ibm.com/docs/en/watsonx/saas?topic=overview-faq', 'title': 'Frequently asked questions'}, page_content=\"What is Watson OpenScale?\\nHow do I convert a prediction column from an integer data type to a categorical data type?\\nWhy does Watson OpenScale need access to training data?\\nWhat does it mean if the fairness score is greater than 100 percent?\\nHow is model bias mitigated by using Watson OpenScale?\\nIs it possible to check for model bias on sensitive attributes, such as race and sex, even when the model is not trained on them?\\nIs it possible to mitigate bias for regression-based models?\\nWhat are the different methods of debiasing in Watson OpenScale?\\nConfiguring a model requires information about the location of the training data and the options are Cloud Object Storage and Db2. If the data is in Netezza, can Watson OpenScale use Netezza?\\nWhy doesn't Watson OpenScale see the updates that were made to the model?\\nWhat are the various kinds of risks associated in using a machine learning model? \\nMust I keep monitoring the Watson OpenScale dashboard to make sure that my models behave as expected?\\nIn Watson OpenScale, what data is used for Quality metrics computation?\"), Document(metadata={'description': 'Find answers to frequently asked questions about watsonx.ai.', 'language': 'en', 'source': 'https://www.ibm.com/docs/en/watsonx/saas?topic=overview-faq', 'title': 'Frequently asked questions'}, page_content='Is it possible to check for model bias on sensitive attributes, such as race and sex, even when the model is not trained on them?\\nYes. Recently, Watson OpenScale delivered a ground-breaking feature called “Indirect Bias detection.” Use it to detect whether the model is exhibiting bias indirectly for sensitive attributes, even though the model is not trained on these\\n              attributes.\\n\\n\\nIs it possible to mitigate bias for regression-based models?\\nYes. You can use Watson OpenScale to mitigate bias on regression-based models. No additional configuration is needed from you to use this feature. Bias mitigation for regression models is done out-of-box when the model exhibits bias.\\n\\n\\nWhat are the different methods of debiasing in Watson OpenScale?\\nYou can use both Active Debiasing and Passive Debiasing for debiasing. For more information, see Debiasing options.'), Document(metadata={'description': 'Find answers to frequently asked questions about watsonx.ai.', 'language': 'en', 'source': 'https://www.ibm.com/docs/en/watsonx/saas?topic=overview-faq', 'title': 'Frequently asked questions'}, page_content=\"Configuring a model requires information about the location of the training data and the options are Cloud Object Storage and Db2. If the data is in Netezza, can Watson OpenScale use Netezza?\\nUse this Watson OpenScale Notebook to read the data\\n              from Netezza and generate the training statistics and also the drift detection model.\\n\\n\\nWhy doesn't Watson OpenScale see the updates that were made to the model?\\nWatson OpenScale works on a deployment of a model, not on the model itself. You must create a new deployment and then configure this new deployment as a new subscription in Watson OpenScale. With this arrangement, you are able to compare the\\n              two versions of the model.\")]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Model bias is mitigated by Watson OpenScale through a two-step process: Learning Phase and Application Phase. In the Learning Phase, Watson OpenScale learns the customer model behavior to understand when it acts in a biased manner. In the Application Phase, Watson OpenScale identifies whether the customer's model acts in a biased manner on a specific data point and, if needed, fixes the bias. Watson OpenScale also offers Indirect Bias detection, which allows for checking model bias on sensitive attributes, such as race and sex, even when the model is not trained on them. Additionally, Watson OpenScale supports bias mitigation for regression-based models.\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"How is model bias mitigated by using Watson OpenScale?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa2f6c4a-5c41-44bc-bc73-a7c3e2d1f255"
   },
   "source": [
    "## Capture the output for logging with watsonx.governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "374d0614-392a-4d75-b1d3-89697626b5e3"
   },
   "outputs": [],
   "source": [
    "prompt_result = {}\n",
    "prompt_result['query'] = response['input']\n",
    "prompt_result['context'] = construct_context(gov_faqs_context)\n",
    "prompt_result['generated_text'] = response['output']\n",
    "complete_content.append(prompt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97deb0ac-1724-4dc6-99bb-64a8a05aa9fb"
   },
   "source": [
    "### Question 2. Related to EU AI Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0fa52925-d372-49fb-9db1-d0de8788e0e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_AI_Act_Summary_Context\",\n",
      "  \"action_input\": {\n",
      "    \"question\": \"What are considered as Prohibited AI systems?\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Users (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\nThis applies to users located in the EU, and third country users where the AI system’s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or closed – must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.\\n\\n\\n\\nProhibited AI systems (Chapter II, Art. 5)\\nThe following types of AI system are ‘Prohibited’ according to the AI Act.\\nAI systems:'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='used as a safety component or a product covered by EU laws in Annex I AND required to undergo a third-party conformity assessment under those Annex I laws; OR\\nthose under Annex III use cases (below), except if:\\n\\nthe AI system performs a narrow procedural task;\\nimproves the result of a previously completed human activity;\\ndetects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or\\nperforms a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III.\\n\\n\\nAI systems are always considered high-risk if it profiles individuals, i.e. automated processing of personaldata to assess various aspects of a person’s life, such as work performance, economic situation, health,preferences, interests, reliability, behaviour, location or movement.\\nProviders whose AI system falls under the use cases in Annex III but believes it is not high-risk must document such anassessment before placing it on the market or putting it into service.'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Unacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\nMost of the text addresses high-risk AI systems, which are regulated.\\nA smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes).\\nMinimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI).\\n\\nThe majority of obligations fall on providers (developers) of high-risk AI systems.\\n\\nThose that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country.\\nAnd also third country providers where the high risk AI system’s output is used in the EU.\\n\\nUsers are natural or legal persons that deploy an AI system in a professional capacity, not affected end-users.'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='General purpose AI (GPAI)\\nGPAI model means an AI model, including when trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable to competently perform a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications. This does not cover AI models that are used before release on the market for research, development and prototyping activities.\\nGPAI system means an AI system which is based on a general purpose AI model, that has the capability to serve a variety of purposes, both for direct use as well as for integration in other AI systems.\\nGPAI systems may be used as high risk AI systems or integrated into them. GPAI system providers should cooperate with such high risk AI system providers to enable the latter’s compliance.\\nAll providers of GPAI models must:')]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Prohibited AI systems, as per the EU Artificial Intelligence Act, are AI systems that are used as a safety component or a product covered by EU laws in Annex I and required to undergo a third-party conformity assessment under those Annex I laws. They also include AI systems under Annex III use cases, except if the AI system performs a narrow procedural task, improves the result of a previously completed human activity, detects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review, or performs a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III. Additionally, AI systems that profile individuals, i.e., automated processing of personal data to assess various aspects of a person’s life, are always considered high-risk. Providers whose AI system falls under the use cases in Annex III but believes it is not high-risk must document such an assessment before placing it on the market or putting it into service.\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"input\": \"What are considered as Prohibited AI systems?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "f58754cb-f602-4d1d-aadb-67593bdeb9fd"
   },
   "outputs": [],
   "source": [
    "prompt_result = {}\n",
    "prompt_result['query'] = response['input']\n",
    "prompt_result['context'] = construct_context(ai_act_context)\n",
    "prompt_result['generated_text'] = response['output']\n",
    "complete_content.append(prompt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03671fad-ede9-4d6d-8ae9-200735aab0e5"
   },
   "source": [
    "### Question 3. Related to EU AI Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "c70f4d78-e5bb-4d9d-97c4-4a1ca47acc57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_AI_Act_Summary_Context\",\n",
      "  \"action_input\": {\n",
      "    \"question\": \"What are considered as High risk AI systems?\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Requirements for providers of high-risk AI systems (Art. 8–17)\\nHigh risk AI providers must:\\n\\nEstablish a risk management system throughout the high risk AI system’s lifecycle;\\nConduct data governance, ensuring that training, validation and testing datasets are relevant, sufficiently representative and, to the best extent possible, free of errors and complete according to the intended purpose.\\nDraw up technical documentation to demonstrate compliance and provide authorities with the information to assess that compliance.\\nDesign their high risk AI system for record-keeping to enable it to automatically record events relevant for identifying national level risks and substantial modifications throughout the system’s lifecycle.\\nProvide instructions for use to downstream deployers to enable the latter’s compliance.\\nDesign their high risk AI system to allow deployers to implement human oversight.\\nDesign their high risk AI system to achieve appropriate levels of accuracy, robustness, and cybersecurity.\\nEstablish a quality management system to ensure compliance.'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Users (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\nThis applies to users located in the EU, and third country users where the AI system’s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\nAll providers of GPAI models that present a systemic risk – open or closed – must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.\\n\\n\\n\\nProhibited AI systems (Chapter II, Art. 5)\\nThe following types of AI system are ‘Prohibited’ according to the AI Act.\\nAI systems:'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Unacceptable risk is prohibited (e.g. social scoring systems and manipulative AI).\\nMost of the text addresses high-risk AI systems, which are regulated.\\nA smaller section handles limited risk AI systems, subject to lighter transparency obligations: developers and deployers must ensure that end-users are aware that they are interacting with AI (chatbots and deepfakes).\\nMinimal risk is unregulated (including the majority of AI applications currently available on the EU single market, such as AI enabled video games and spam filters – at least in 2021; this is changing with generative AI).\\n\\nThe majority of obligations fall on providers (developers) of high-risk AI systems.\\n\\nThose that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU or a third country.\\nAnd also third country providers where the high risk AI system’s output is used in the EU.\\n\\nUsers are natural or legal persons that deploy an AI system in a professional capacity, not affected end-users.'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Notes on remote biometric identification:\\nUsing AI-enabled real-time RBI is only allowed when not using the tool would cause considerable harm and must account for affected persons’ rights and freedoms.\\nBefore deployment, police must complete a fundamental rights impact assessment and register the system in the EU database, though, in duly justified cases of urgency, deployment can commence without registration, provided that it is registered later without undue delay.\\nBefore deployment, they also must obtain authorisation from a judicial authority or independent administrative authority[1], though, in duly justified cases of urgency, deployment can commence without authorisation, provided that authorisation is requested within 24 hours. If authorisation is rejected, deployment must cease immediately, deleting all data, results, and outputs.\\n↲ [1] Independent administrative authorities may be subject to greater political influence than judicial authorities (Hacker, 2024).\\n\\nHigh risk AI systems (Chapter III)\\nSome AI systems are considered ‘High risk’ under the AI Act. Providers of those systems will be subject to additional requirements.\\nClassification rules for high-risk AI systems (Art. 6)\\nHigh risk AI systems are those:')]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"High risk AI systems are those: (1) used as a safety component or a product covered by EU laws in Annex I and required to undergo a third-party conformity assessment under those Annex I laws, (2) AI systems under Annex III use cases, except if the AI system performs a narrow procedural task, improves the result of a previously completed human activity, detects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review, or performs a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III, and (3) AI systems that profile individuals, i.e., automated processing of personal data to assess various aspects of a person’s life.\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"input\": \"What are considered as High risk AI systems?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3f18e5cb-9d2b-4c89-bff9-f02673c20cbf"
   },
   "outputs": [],
   "source": [
    "prompt_result = {}\n",
    "prompt_result['query'] = response['input']\n",
    "prompt_result['context'] = construct_context(ai_act_context)\n",
    "prompt_result['generated_text'] = response['output']\n",
    "complete_content.append(prompt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76441842-2ef4-4a21-bb72-3a58dc3f85fe"
   },
   "source": [
    "### Question 4. Related to watsonx.gov FAQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "44c97062-61c7-4709-a7c9-dc5e9d0c8488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "AI:\n",
      "Thought: The user is asking about monitoring custom metrics with Watson OpenScale. I will use the get_AI_Act_Summary_Context tool to provide an accurate response.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_AI_Act_Summary_Context\",\n",
      "  \"action_input\": \"How to monitor custom metrics with Watson OpenScale\"\n",
      "}\n",
      "```\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Requirements for providers of high-risk AI systems (Art. 8–17)\\nHigh risk AI providers must:\\n\\nEstablish a risk management system throughout the high risk AI system’s lifecycle;\\nConduct data governance, ensuring that training, validation and testing datasets are relevant, sufficiently representative and, to the best extent possible, free of errors and complete according to the intended purpose.\\nDraw up technical documentation to demonstrate compliance and provide authorities with the information to assess that compliance.\\nDesign their high risk AI system for record-keeping to enable it to automatically record events relevant for identifying national level risks and substantial modifications throughout the system’s lifecycle.\\nProvide instructions for use to downstream deployers to enable the latter’s compliance.\\nDesign their high risk AI system to allow deployers to implement human oversight.\\nDesign their high risk AI system to achieve appropriate levels of accuracy, robustness, and cybersecurity.\\nEstablish a quality management system to ensure compliance.'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Free and open licence GPAI models – whose parameters, including weights, model architecture and model usage are publicly available, allowing for access, usage, modification and distribution of the model – only have to comply with the latter two obligations above, unless the free and open licence GPAI model is systemic.\\nGPAI models present systemic risks when the cumulative amount of compute used for its training is greater than 1025 floating point operations (FLOPs). Providers must notify the Commission if their model meets this criterion within 2 weeks. The provider may present arguments that, despite meeting the criteria, their model does not present systemic risks. The Commission may decide on its own, or via a qualified alert from the scientific panel of independent experts, that a model has high impact capabilities, rendering it systemic.\\nIn addition to the four obligations above, providers of GPAI models with systemic risk must also:\\n\\nPerform model evaluations, including conducting and documenting adversarial testing to identify and mitigate systemic risk.\\nAssess and mitigate possible systemic risks, including their sources.\\nTrack, document and report serious incidents and possible corrective measures to the AI Office and relevant national competent authorities without undue delay.\\nEnsure an adequate level of cybersecurity protection.'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='deploying subliminal, manipulative, or deceptive techniques to distort behaviour and impair informed decision-making, causing significant harm.\\nexploiting vulnerabilities related to age, disability, or socio-economic circumstances to distort behaviour, causing significant harm.\\nbiometric categorisation systems inferring sensitive attributes (race, political opinions, trade union membership, religious or philosophical beliefs, sex life, or sexual orientation), except labelling or filtering of lawfully acquired biometric datasets or when law enforcement categorises biometric data.\\nsocial scoring, i.e., evaluating or classifying individuals or groups based on social behaviour or personal traits, causing detrimental or unfavourable treatment of those people.\\nassessing the risk of an individual committing criminal offenses solely based on profiling or personality traits, except when used to augment human assessments based on objective, verifiable facts directly linked to criminal activity.\\ncompiling facial recognition databases by untargeted scraping of facial images from the internet or CCTV footage.\\ninferring emotions in workplaces or educational institutions, except for medical or safety reasons.\\n‘real-time’ remote biometric identification (RBI) in publicly accessible spaces for law enforcement, except when:'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='The AI Office will be established, sitting within the Commission, to monitor the effective implementation and compliance of GPAI model providers.\\nDownstream providers can lodge a complaint regarding the upstream providers infringement to the AI Office.\\nThe AI Office may conduct evaluations of the GPAI model to:\\n\\nassess compliance where the information gathered under its powers to request information is insufficient.\\nInvestigate systemic risks, particularly following a qualified report from the scientific panel of independent experts.\\n\\n\\n\\n\\nTimelines\\n\\nAfter entry into force, the AI Act will apply by the following deadlines:\\n\\n6 months for prohibited AI systems.\\n12 months for GPAI.\\xa0\\n24 months for high risk AI systems under Annex III.\\xa0\\n36 months for high risk AI systems under Annex I.\\n\\n\\nCodes of practice must be ready 9 months after entry into force.\\xa0\\n\\nSee our full implementation timeline for all key milestones relating to the implementation of the AI Act.\\n\\n\\nThis post was published on 27 Feb, 2024\\n\\n\\n\\n\\n\\n\\n\\nRelated articles\\n\\n\\n\\n\\nJob Opportunities at the European AI Office for Legal and Policy Backgrounds')]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"To monitor custom metrics with Watson OpenScale, you can use the Watson OpenScale API to collect and analyze data on your AI models' performance. This includes metrics such as accuracy, fairness, and explainability. You can also set up alerts to notify you when certain thresholds are met. For more detailed information, refer to the Watson OpenScale documentation.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"input\": \"How to monitor custom metrics with Watson OpenScale?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "e59856d8-9b97-4966-ad71-224ca19849ef"
   },
   "outputs": [],
   "source": [
    "prompt_result = {}\n",
    "prompt_result['query'] = response['input']\n",
    "prompt_result['context'] = construct_context(gov_faqs_context)\n",
    "prompt_result['generated_text'] = response['output']\n",
    "complete_content.append(prompt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "b14d579a-4fea-4ef0-af81-f00bec6ea0a0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "llm_data = pd.DataFrame(complete_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "16a119f4-7bf9-485c-be6e-823d9530152c"
   },
   "outputs": [],
   "source": [
    "llm_data['reference'] = llm_data['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "35735ae0-d03b-41cd-97f6-e04c6ee39fdf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is model bias mitigated by using Watson Op...</td>\n",
       "      <td>What does it mean if the fairness score is gre...</td>\n",
       "      <td>Model bias is mitigated by Watson OpenScale th...</td>\n",
       "      <td>Model bias is mitigated by Watson OpenScale th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are considered as Prohibited AI systems?</td>\n",
       "      <td>Users (deployers) of high-risk AI systems have...</td>\n",
       "      <td>Prohibited AI systems, as per the EU Artificia...</td>\n",
       "      <td>Prohibited AI systems, as per the EU Artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are considered as High risk AI systems?</td>\n",
       "      <td>Requirements for providers of high-risk AI sys...</td>\n",
       "      <td>High risk AI systems are those: (1) used as a ...</td>\n",
       "      <td>High risk AI systems are those: (1) used as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to monitor custom metrics with Watson Open...</td>\n",
       "      <td>What does it mean if the fairness score is gre...</td>\n",
       "      <td>To monitor custom metrics with Watson OpenScal...</td>\n",
       "      <td>To monitor custom metrics with Watson OpenScal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  How is model bias mitigated by using Watson Op...   \n",
       "1      What are considered as Prohibited AI systems?   \n",
       "2       What are considered as High risk AI systems?   \n",
       "3  How to monitor custom metrics with Watson Open...   \n",
       "\n",
       "                                             context  \\\n",
       "0  What does it mean if the fairness score is gre...   \n",
       "1  Users (deployers) of high-risk AI systems have...   \n",
       "2  Requirements for providers of high-risk AI sys...   \n",
       "3  What does it mean if the fairness score is gre...   \n",
       "\n",
       "                                      generated_text  \\\n",
       "0  Model bias is mitigated by Watson OpenScale th...   \n",
       "1  Prohibited AI systems, as per the EU Artificia...   \n",
       "2  High risk AI systems are those: (1) used as a ...   \n",
       "3  To monitor custom metrics with Watson OpenScal...   \n",
       "\n",
       "                                           reference  \n",
       "0  Model bias is mitigated by Watson OpenScale th...  \n",
       "1  Prohibited AI systems, as per the EU Artificia...  \n",
       "2  High risk AI systems are those: (1) used as a ...  \n",
       "3  To monitor custom metrics with Watson OpenScal...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6515fa07-5b1a-4a53-82c5-e34266b4fc21"
   },
   "source": [
    "# Computing Answer Quality and Retrieval Quality Metrics using IBM watsonx.governance for RAG task\n",
    "\n",
    "This notebook demonstrates the creation of a Retrieval Augumented Generation (RAG) pattern using watsonx.ai and computations of reference-free Answer Quality metrics, such as **Faithfulness**, **Answer relevance**, **Unsuccessful requests**, Content Analysis metrics such as **Coverage**, **Density**, **Abstractness** and Retrieval Quality Metrics such as **Context Relevance**, **Retrieval Precision**, **Average Precision**, **Reciprocal Rank**, **Hit Rate** and **Normalized Discounted Cumulative Gain** for the RAG task type. It also identifies the source attribution using watsonx.governance.\n",
    "\n",
    "- **Faithfulness** measures how faithful the model output or generated text is to the context sent to the LLM input. The faithfulness score is a value between 0 and 1. A value closer to 1 indicates that the output is more faithful - or grounded - and less hallucinated. A value closer to 0 indicates that the output is less faithful and more hallucinated.\n",
    "\n",
    "- **Answer relevance** measures how relevant the answer or generated text is to the question. This is one of the ways to determine the quality of your model. The answer relevance score is a value between 0 and 1. A value closer to 1 indicates that the answer is more relevant to the given question. A value closer to 0 indicates that the answer is less relevant to the question.\n",
    "\n",
    "- **Unsuccessful requests** measures the ratio of questions answered unsuccessfully out of the total number of questions. The unsuccessful requests score is a value between 0 and 1. A value closer to 0 indicates that the model is successfully answering the questions. A value closer to 1 indicates the model is not able to answer the questions.\n",
    "\n",
    "- **Coverage** metric quantifies the extent to which the model output. It measures the percentage of output words that are also in the input text. The coverage score is a value between 0 and 1. A higher score close to 1 indicates that higher percentage of output words are within the input text.\n",
    "\n",
    "- **Density** quantifies how well the summary or the answer in the model output can be described as a series of extractions from the model input or context. A lower value of density metric indicates that on average the extractive fragments do not closely resemble verbatim extractions from the original source or context text. Abstractive summarization involves generating new, concise sentences that convey the main ideas of the source text but may not be directly copied from it. This is in contrast to extractive summarization, where the summary consists mainly of sentences directly lifted from the source. The lower the score the more abstractive the model output is.\n",
    "\n",
    "- **Abstractness** measures the abstractness of the model generated text by measuring the new n-grams generated in the model output compared to the model input. The metric computes the ratio of n-grams in the generated text that do not appear in the source content or context send to the model. The abstractness score is a value between 0 and 1. A higher score close to 1 indicates high abstractness in the generated text.\n",
    "\n",
    "- **Context Relevance** assesses the degree to which the retrieved context is relevant to the question sent to the LLM. This is one of the ways to determine the quality of your retrieval system. The context relevance score is a value between 0 and 1. A value closer to 1 indicates that the context is more relevant to your question in the prompt. A value closer to 0 indicates that the context is less relevant to your question in the prompt.\n",
    "\n",
    "- **Retrieval Precision** measures the quantity of relevant contexts from the total contexts retrieved. The retrieval precision is a value between 0 and 1. A value of 1 indicates that all the retrieved contexts are relevant. A value of 0 indicates that none of the retrieved contexts are relevant.\n",
    "\n",
    "- **Average Precision** evaluates whether all the relevant contexts are ranked higher or not. It is the mean of the precision scores of relevant contexts. The average precision is a value between 0 and 1. A value of 1 indicates that all the relevant contexts are ranked higher. A value of 0 indicates that none of the retrieved contexts are relevant.\n",
    "\n",
    "- **Reciprocal Rank** is the reciprocal of the rank of the first relevant context. The retrieval reciprocal rank is a value between 0 and 1. A value of 1 indicates that the first relevant context is at first position. A value of 0 indicates that none of the relevant contexts are retrieved.\n",
    "\n",
    "- **Hit Rate** Hit Rate measures whether there is atleast one relevant context among the retrieved contexts. The hit rate value is either 0 or 1. A value of 1 indicates that there is at least one relevant context. A value of 0 indicates that there is no relevant context in the retrieved contexts.\n",
    "\n",
    "- **Normalized Discounted Cumulative Gain** Normalized Discounted Cumulative Gain or NDCG measures the ranking quality of the retrieved contexts. The ndcg is a value between 0 and 1. A value of 1 indicates that the retrieved contexts are ranked in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "8077b3d9-a583-4814-9cfb-27ac1bdbc04b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.25'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from ibm_watsonx_ai import APIClient\n",
    "\n",
    "wml_client = APIClient(credentials)\n",
    "wml_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3bebb68-8c3c-46f3-8c30-c6926fde5d0d"
   },
   "source": [
    "### Function to create the access token\n",
    "This function generates an IAM access token using the provided credentials. The API calls for creating and scoring prompt template assets utilize the token generated by this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1c33e5ab-54fe-40b7-b0b9-f81f17925c6c"
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "def generate_access_token():\n",
    "    headers={}\n",
    "    headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n",
    "    headers[\"Accept\"] = \"application/json\"\n",
    "    data = {\n",
    "        \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n",
    "        \"apikey\": CLOUD_API_KEY,\n",
    "        \"response_type\": \"cloud_iam\"\n",
    "    }\n",
    "    response = requests.post(IAM_URL + \"/identity/token\", data=data, headers=headers)\n",
    "    json_data = response.json()\n",
    "    iam_access_token = json_data[\"access_token\"]\n",
    "        \n",
    "    return iam_access_token\n",
    "\n",
    "iam_access_token = generate_access_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "898372f6-0411-486d-82ac-461c2a817981"
   },
   "outputs": [],
   "source": [
    "test_data_path = \"RAG_data.csv\"\n",
    "llm_data.to_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "137766fc-72aa-4ce9-b2b1-bb307c44b434"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>context</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How is model bias mitigated by using Watson Op...</td>\n",
       "      <td>What does it mean if the fairness score is gre...</td>\n",
       "      <td>Model bias is mitigated by Watson OpenScale th...</td>\n",
       "      <td>Model bias is mitigated by Watson OpenScale th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are considered as Prohibited AI systems?</td>\n",
       "      <td>Users (deployers) of high-risk AI systems have...</td>\n",
       "      <td>Prohibited AI systems, as per the EU Artificia...</td>\n",
       "      <td>Prohibited AI systems, as per the EU Artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are considered as High risk AI systems?</td>\n",
       "      <td>Requirements for providers of high-risk AI sys...</td>\n",
       "      <td>High risk AI systems are those: (1) used as a ...</td>\n",
       "      <td>High risk AI systems are those: (1) used as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to monitor custom metrics with Watson Open...</td>\n",
       "      <td>What does it mean if the fairness score is gre...</td>\n",
       "      <td>To monitor custom metrics with Watson OpenScal...</td>\n",
       "      <td>To monitor custom metrics with Watson OpenScal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  How is model bias mitigated by using Watson Op...   \n",
       "1      What are considered as Prohibited AI systems?   \n",
       "2       What are considered as High risk AI systems?   \n",
       "3  How to monitor custom metrics with Watson Open...   \n",
       "\n",
       "                                             context  \\\n",
       "0  What does it mean if the fairness score is gre...   \n",
       "1  Users (deployers) of high-risk AI systems have...   \n",
       "2  Requirements for providers of high-risk AI sys...   \n",
       "3  What does it mean if the fairness score is gre...   \n",
       "\n",
       "                                      generated_text  \\\n",
       "0  Model bias is mitigated by Watson OpenScale th...   \n",
       "1  Prohibited AI systems, as per the EU Artificia...   \n",
       "2  High risk AI systems are those: (1) used as a ...   \n",
       "3  To monitor custom metrics with Watson OpenScal...   \n",
       "\n",
       "                                           reference  \n",
       "0  Model bias is mitigated by Watson OpenScale th...  \n",
       "1  Prohibited AI systems, as per the EU Artificia...  \n",
       "2  High risk AI systems are those: (1) used as a ...  \n",
       "3  To monitor custom metrics with Watson OpenScal...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "981e0b1d-b50c-47c9-aae9-4feaf5b99b27"
   },
   "outputs": [],
   "source": [
    "from ibm_aigov_facts_client import AIGovFactsClient\n",
    "\n",
    "facts_client = AIGovFactsClient(\n",
    "    api_key=CLOUD_API_KEY,\n",
    "    container_id=project_id,\n",
    "    container_type=\"project\",\n",
    "    disable_tracing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7d29366f-bc1e-42dc-bdb2-16bc158ec41d"
   },
   "source": [
    "### Create Detached Prompt template\n",
    "\n",
    "Create a prompt template for a retrieval augmented generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9d5eecf5-6446-4614-bf4c-1580beac1155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024/12/31 14:37:09 INFO : ------------------------------ Detached Prompt Creation Started ------------------------------\n",
      "2024/12/31 14:37:11 INFO : The detached prompt with ID 52c3f9d5-9db3-4b5e-a33c-7947a5e8a576 was created successfully in container_id f1774d00-d497-459c-b31f-74d588bf16e4.\n"
     ]
    }
   ],
   "source": [
    "from ibm_aigov_facts_client import DetachedPromptTemplate, PromptTemplate\n",
    "\n",
    "detached_information = DetachedPromptTemplate(\n",
    "    prompt_id=\"detached_prompt\",\n",
    "    model_id=\"ibm/granite-3-8b-instruct\",\n",
    "    model_provider=\"IBM\",\n",
    "    model_name=\"granite-3-8b-instruct\",\n",
    "    model_url=\"model_url\",\n",
    "    prompt_url=\"prompt_url\",\n",
    "    prompt_additional_info={\"IBM Cloud Region\": \"us-east1\"}\n",
    ")\n",
    "\n",
    "task_id = \"retrieval_augmented_generation\"\n",
    "name = \"Agentic RAG Testing\"\n",
    "description = \"Agentic RAG Testing\"\n",
    "model_id = \"ibm/granite-3-8b-instruct\"\n",
    "\n",
    "# define parameters for PromptTemplate\n",
    "prompt_variables = {\"context\": \"\", \"query\": \"\"}\n",
    "input = prompt_input\n",
    "input_prefix= \"\"\n",
    "output_prefix= \"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input=input,\n",
    "    prompt_variables=prompt_variables,\n",
    "    input_prefix=input_prefix,\n",
    "    output_prefix=output_prefix,\n",
    ")\n",
    "\n",
    "pta_details = facts_client.assets.create_detached_prompt(\n",
    "    model_id=model_id,\n",
    "    task_id=task_id,\n",
    "    name=name,\n",
    "    description=description,\n",
    "    prompt_details=prompt_template,\n",
    "    detached_information=detached_information)\n",
    "project_pta_id = pta_details.to_dict()[\"asset_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bacbe5d-fa65-4fa1-9be4-6667b8777cef"
   },
   "source": [
    "### Configure watsonx.governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "93b76acb-f9ea-41fb-b44c-c1dc331ca491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.41\n"
     ]
    }
   ],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, CloudPakForDataAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "service_instance_id = None # Update this to refer to a particular service instance\n",
    "authenticator = IAMAuthenticator(\n",
    "    apikey=CLOUD_API_KEY,\n",
    "    url=IAM_URL\n",
    ")\n",
    "wos_client = APIClient(\n",
    "    authenticator=authenticator,\n",
    "    service_url=SERVICE_URL,\n",
    "    service_instance_id=service_instance_id\n",
    ")\n",
    "data_mart_id = wos_client.service_instance_id\n",
    "print(wos_client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29e746cf-2148-4c3d-a44a-e6acd09f96ce"
   },
   "source": [
    "### For computing answer quality and retrieval quality metrics you are required to configure LLM As Judge.\n",
    "\n",
    "To compute metrics using LLM As Judge a generative_ai_evaluator integrated system should to be created and provided during prompt setup.\n",
    "\n",
    "#### Create a Generative AI evaluator\n",
    "The Generative AI Evaluator can be any model from watsonx.ai or a custom endpoint invoking external models\n",
    "\n",
    "Supported evaluator types\n",
    "* watsonx.ai (for connecting to watsonx.ai in local Cloud)\n",
    "* custom (for connecting to any external models)\n",
    "\n",
    "#### Integrated System parameters\n",
    "| Parameter | Description |\n",
    "|:-|:-|\n",
    "| `name` | Name for the evaluator. |\n",
    "| `description` | Description for the evaluator. |\n",
    "| `type` | The type of integrated system. Provide `generative_ai_evaluator`. |\n",
    "| `parameters` | The evaluator configuration details like `evaluator_type` and `model_id`. |\n",
    "| `credentials` | The user credentials |\n",
    "| `connection` [Optional]| The scoring endpoint details when the evaluator is of type `custom`. |\n",
    "\n",
    "As an example, an evaluator using FLAN_UL2 model from watsonx.ai instance present in the same Cloud is created below. The other models which can be used from watsonx.ai are FLAN_T5_XXL, FLAN_UL2, FLAN_T5_XL, MIXTRAL_8X7B_INSTRUCT_V01.\n",
    "For more details on the parameters and to create the other supported evaluators please refer to the link [Generative AI evaluator templates](https://github.ibm.com/aiopenscale/notebooks/wiki/Generative-AI-Evaluator-templates#for-ibm-watsonxgovernance-in-cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "275e6840-0aa2-461f-af1e-da371297b57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ba7dd566-93ba-4295-92e7-a37f6d8a16ee'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "gen_ai_evaluator = wos_client.integrated_systems.add(\n",
    "    name=\"RAG Metrics Evaluator\",\n",
    "    description=\"RAG Metrics Evaluator\",\n",
    "    type=\"generative_ai_evaluator\",\n",
    "    parameters={\"evaluator_type\": \"watsonx.ai\", \"model_id\": ModelTypes.MIXTRAL_8X7B_INSTRUCT_V01.value},\n",
    "    credentials={\n",
    "        \"wml_location\": \"cloud\",\n",
    "        \"apikey\": CLOUD_API_KEY,\n",
    "    },\n",
    ")\n",
    "\n",
    "# get evaluator integrated system ID\n",
    "result = gen_ai_evaluator.result._to_dict()\n",
    "evaluator_id = result[\"metadata\"][\"id\"]\n",
    "evaluator_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6502e690-a0eb-49a4-b23e-f5772d68f70b"
   },
   "source": [
    "#### Generative AI monitor parameters\n",
    "\n",
    "##### Generative AI evaluator parameters\n",
    "The generative_ai_evaluator details can be provided at the global level under `generative_ai_quality.parameters` to use the same evaluator for all the Answer quality metrics(Faithfulness, Answer relevance, Answer similarity) and Retrieval quality metrics(Context relevance, Retrieval precision, Average precision, Reciprocal rank, Hit rate, Normalized Discounted Cumulative Gain).\n",
    "\n",
    "The generative ai evaluator can be specified at metric level to use different evaluators for each of the metric. The metrics under which generative_ai_evaluator parameter can be specified are faithfulness, answer_relevance, answer_similarity and retrieval_quality. The generative_ai_evaluator at the metric level takes precedence over the generative_ai_evaluator at global level\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "|:-|:-|:-|\n",
    "| `enabled`| The flag to enable generative ai evaluator |  |\n",
    "| `evaluator_id`| The id of the generative ai evaluator integrated system. |  |\n",
    "\n",
    "##### Faithfulness, Context relevance, Answer relevance and Answer similarity parameters\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "|:-|:-|:-|\n",
    "| `metric_prompt_template` [Optional]| The prompt template used to compute each metric values. User can override the prompt template used by watsonx.governance to compute the metric using this parameter. The prompt template should use the variables {context}, {question}, {answer}, {reference_answer} as needed and these variable values will be filled with the actual data while calling the scoring function. The prompt response should return the metric value in the range 1-10 for the respective metric and in one of the formats [\"4\", \"7 star\", \"star: 8\", \"stars: 9\"] as answer. |  |\n",
    "\n",
    "##### Unsuccessful requests parameters\n",
    "\n",
    "| Parameter | Description | Default Value |\n",
    "|:-|:-|:-|\n",
    "| `unsuccessful_phrases` [Optional]| The list of phrases to be used for comparing the model output to determine whether the request is unsuccessful or not. | `[\"i don't know\", \"i do not know\", \"i'm not sure\", \"i am not sure\", \"i'm unsure\", \"i am unsure\", \"i'm uncertain\", \"i am uncertain\", \"i'm not certain\", \"i am not certain\", \"i can't fulfill\", \"i cannot fulfill\"]` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a628b3f-21cd-4ff0-9271-97db5e84bffd"
   },
   "source": [
    "## Configure Answer Quality metrics & Retrieval Quality metrics\n",
    "<a id=\"config\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93eeb5fd-096a-41a8-a7f5-24eb65f40245"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41c693ff-2f92-47cd-882a-06cc83b3e2fb"
   },
   "source": [
    "#### Common parameters\n",
    "\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "|:-|:-|:-|:-|\n",
    "| context_columns | The list of context column names in the input data frame. |  |  |\n",
    "| question_column | the name of the question column in the input data frame. |  |  |\n",
    "| answer_column | The name of the answer column in the input data frame |  |  |\n",
    "| record_level | The flag to return the record level metrics values. Set the flag under configuration to generate record level metrics for all the metrics. Set the flag under specific metric to generate record level metrics for that metric alone. | `False` | `True`, `False` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7159495a-6ae2-4569-b588-22f74398081a"
   },
   "source": [
    "#### Faithfulness parameters\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "|:-|:-|:-|:-|\n",
    "| attributions_count [Optional]| Source attributions are computed for each sentence in the generated answer. Source attribution for a sentence is the set of sentences in the context which contributed to the LLM generating that sentence in the answer.  The attributions_count parameter specifies the number of sentences in the context which need to be identified for attributions.  E.g., if the value is set to 2, then we will find the top 2 sentences from the context as source attributions. | `3` |  |\n",
    "| ngrams [Optional]| The number of sentences to be grouped from the context when computing faithfulness score. These grouped sentences will be shown in the attributions. Having a very high value of ngrams might lead to having lower faithfulness scores due to dispersion of data and inclusion of unrelated sentences in the attributions. Having a very low value might lead to increase in metric computation time and attributions not capturing the all the aspects of the answer. | `2` |  |\n",
    "| sample_size [Optional]| The faithfulness metric is computed for a maximum of 50 LLM responses.  If you wish to compute it for a smaller number of responses, set the sample_size value to a lower number. If the number of records in the input data frame are more than the sample size, a uniform random sample will taken for computation. | `50` | Integer between 0 to 50. Max value supported is 50 |\n",
    "| record_level [Optional]| Set the flag to generate record level metrics for the specific metric. | `False` | `True`, `False` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "514eb681-aee3-42db-b9d5-43dfb0c012e0"
   },
   "source": [
    "#### Context relevance parameters\n",
    "| Parameter | Description | Default Value | Possible Value(s) |\n",
    "|:-|:-|:-|:-|\n",
    "| ngrams [Optional]| The number of sentences to be grouped from the context when computing context relevance score. Having a very high value of ngrams might lead to having lower context relevance scores due to dispersion of data and inclusion of unrelated sentences. | `5` |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9235345-55f2-4fee-81c7-0cf8620507b4"
   },
   "source": [
    "\n",
    "#### Unsuccessful requests parameters\n",
    "| Parameter | Description | Default Value |\n",
    "|:-|:-|:-|\n",
    "| unsuccessful_phrases [Optional]| The list of phrases to be used for comparing the model output to determine whether the request is unsuccessful or not. | `[\"i don't know\", \"i do not know\", \"i'm not sure\", \"i am not sure\", \"i'm unsure\", \"i am unsure\", \"i'm uncertain\", \"i am uncertain\", \"i'm not certain\", \"i am not certain\", \"i can't fulfill\", \"i cannot fulfill\"]` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7b6546b-f3cf-4264-a249-0a0bb273970f"
   },
   "source": [
    "### Configure faithfulness, answer relevance, and unsuccessful requests parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "58edc161-6e5e-4034-b992-a2ee4b86082f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "This method will be deprecated in the next release and be replaced by wos_client.wos.execute_prompt_setup() method\n",
      "=================================================================\n",
      "\n",
      "\n",
      "=============================================================================\n",
      "\n",
      " Waiting for end of adding prompt setup 52c3f9d5-9db3-4b5e-a33c-7947a5e8a576 \n",
      "\n",
      "=============================================================================\n",
      "\n",
      "\n",
      "\n",
      "running..\n",
      "finished\n",
      "\n",
      "---------------------------------------------------------------\n",
      " Successfully finished setting up prompt template subscription \n",
      "---------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt_template_asset_id': '52c3f9d5-9db3-4b5e-a33c-7947a5e8a576',\n",
       " 'project_id': 'f1774d00-d497-459c-b31f-74d588bf16e4',\n",
       " 'deployment_id': '1de4eb8d-849d-4772-9b4a-ae23f6937e0a',\n",
       " 'service_provider_id': 'c5fef60d-3309-47c3-b044-c40a6936ad6d',\n",
       " 'subscription_id': '57221ce7-a6a0-4b80-a08c-0268c56631cd',\n",
       " 'mrm_monitor_instance_id': '2a3b1d9a-42e4-492f-a09e-084f4f33a306',\n",
       " 'start_time': '2024-12-31T14:37:17.062585Z',\n",
       " 'end_time': '2024-12-31T14:37:34.416446Z',\n",
       " 'status': {'state': 'FINISHED'}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_column = \"reference\"\n",
    "context_fields = [\"context\"]\n",
    "question_field = \"query\"\n",
    "operational_space_id = \"development\"\n",
    "problem_type= \"retrieval_augmented_generation\"\n",
    "input_data_type= \"unstructured_text\"\n",
    "\n",
    "monitors = {\n",
    "    \"generative_ai_quality\": {\n",
    "        \"parameters\": {\n",
    "            \"generative_ai_evaluator\": { # global LLM as judge configuration\n",
    "               \"enabled\": True,\n",
    "               \"evaluator_id\": evaluator_id,\n",
    "            },\n",
    "            \"min_sample_size\": 2,\n",
    "            \"metrics_configuration\": {\n",
    "                \"faithfulness\": {\n",
    "                    # \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric.\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                },\n",
    "                \"answer_relevance\": {\n",
    "                    # \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                },\n",
    "                \"rouge_score\": {},\n",
    "                \"exact_match\": {},\n",
    "                \"bleu\": {},\n",
    "                \"unsuccessful_requests\": {\n",
    "                    # \"unsuccessful_phrases\": []\n",
    "                },\n",
    "                \"hap_input_score\": {},\n",
    "                \"hap_score\": {},\n",
    "                \"pii\": {},\n",
    "                \"pii_input\": {},\n",
    "                \"retrieval_quality\": {\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                    # The metrics computed for retrieval quality are context_relevance, retrieval_precision, average_precision, reciprocal_rank, hit_rate, normalized_discounted_cumulative_gain\n",
    "                    # \"context_relevance\": {\n",
    "                    #     \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # }\n",
    "                },\n",
    "                # Answer similarity metric is supported only when LLM as judge is configured. Uncomment only when using LLM as judge.\n",
    "                \"answer_similarity\": {\n",
    "                    # \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = wos_client.monitor_instances.mrm.execute_prompt_setup(\n",
    "    prompt_template_asset_id=project_pta_id, \n",
    "    project_id=project_id,\n",
    "    label_column=label_column,\n",
    "    context_fields = context_fields,     \n",
    "    question_field = question_field,     \n",
    "    operational_space_id=operational_space_id, \n",
    "    problem_type=problem_type,\n",
    "    input_data_type=input_data_type, \n",
    "    supporting_monitors=monitors, \n",
    "    background_mode=False\n",
    ")\n",
    "\n",
    "result = response.result\n",
    "result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c129e9d5-52b9-4c78-8385-ace144617eb9"
   },
   "source": [
    "With the following cell, you can read the prompt setup task and check its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "4d903cbc-64d3-4be8-bd6d-75ed808aa276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "This method will be deprecated in the next release and be replaced by wos_client.wos.get_prompt_setup() method\n",
      "=================================================================\n",
      "Finished prompt setup. The response is {'prompt_template_asset_id': '52c3f9d5-9db3-4b5e-a33c-7947a5e8a576', 'project_id': 'f1774d00-d497-459c-b31f-74d588bf16e4', 'deployment_id': '1de4eb8d-849d-4772-9b4a-ae23f6937e0a', 'service_provider_id': 'c5fef60d-3309-47c3-b044-c40a6936ad6d', 'subscription_id': '57221ce7-a6a0-4b80-a08c-0268c56631cd', 'mrm_monitor_instance_id': '2a3b1d9a-42e4-492f-a09e-084f4f33a306', 'start_time': '2024-12-31T14:37:17.062585Z', 'end_time': '2024-12-31T14:37:34.416446Z', 'status': {'state': 'FINISHED'}}\n"
     ]
    }
   ],
   "source": [
    "response = wos_client.monitor_instances.mrm.get_prompt_setup(\n",
    "    prompt_template_asset_id=project_pta_id,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "result = response.result\n",
    "result_json = result.to_dict()\n",
    "\n",
    "if result_json[\"status\"][\"state\"] == \"FINISHED\":\n",
    "    print(\"Finished prompt setup. The response is {}\".format(result_json))\n",
    "else:\n",
    "    print(\"Prompt setup failed. The response is {}\".format(result_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "b2d49409-24cf-42c1-9982-f3c474b829a3"
   },
   "outputs": [],
   "source": [
    "subscription_id = result_json[\"subscription_id\"]\n",
    "mrm_monitor_instance_id = result_json[\"mrm_monitor_instance_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56709bff-f7d6-4d57-aac6-836bedb32599"
   },
   "source": [
    "### Show all monitor instances in the development subscription\n",
    "The following cell lists the monitors present in the development subscription, along with their respective statuses and other details. Please wait for all the monitors to be in an active state before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ab997dfb-c8a9-45b9-b977-ec7fc860702f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<HTML>\n",
       "        <body>\n",
       "            <h3>Monitor instances</h3>\n",
       "            <table style='border: 1px solid #dddddd; font-family: Courier'>\n",
       "                <th style='border: 1px solid #dddddd'>data_mart_id</th><th style='border: 1px solid #dddddd'>status</th><th style='border: 1px solid #dddddd'>target_id</th><th style='border: 1px solid #dddddd'>target_type</th><th style='border: 1px solid #dddddd'>monitor_definition_id</th><th style='border: 1px solid #dddddd'>created_at</th><th style='border: 1px solid #dddddd'>id</th>\n",
       "                <tr><td style='border: 1px solid #dddddd'>1682cdfd-5f26-467d-8597-07660c2b02df</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>2024-12-31 14:37:25.108000+00:00</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td></tr><tr><td style='border: 1px solid #dddddd'>1682cdfd-5f26-467d-8597-07660c2b02df</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>model_health</td><td style='border: 1px solid #dddddd'>2024-12-31 14:37:26.145000+00:00</td><td style='border: 1px solid #dddddd'>13b8e17d-f442-4fb8-9b32-fdcd6fa371c7</td></tr><tr><td style='border: 1px solid #dddddd'>1682cdfd-5f26-467d-8597-07660c2b02df</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>mrm</td><td style='border: 1px solid #dddddd'>2024-12-31 14:37:27.203000+00:00</td><td style='border: 1px solid #dddddd'>2a3b1d9a-42e4-492f-a09e-084f4f33a306</td></tr>\n",
       "            </table>\n",
       "        </body>\n",
       "        </HTML>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wos_client.monitor_instances.show(target_target_id=subscription_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c8768ff-b8eb-4d33-9996-941d70343a47"
   },
   "source": [
    "### Risk evaluations for the PTA subscription - Evaluate the prompt template subscription\n",
    "\n",
    "For risk assessment of a `development`-type subscription, you must have an evaluation dataset. The risk assessment function takes the evaluation dataset path as a parameter when evaluating the configured metrics. If there is a discrepancy between the feature columns in the subscription and the column names in the uploading `.CSV` file, you have the option to supply a mapping JSON file to associate the `.CSV` column names with the feature column names in the subscription.\n",
    "\n",
    "**Note**: If you are running this notebook from Watson Studio, you may first need to upload your test data to Watson Studio, then run the code snippet below to download the feedback data file from the project to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "51072101-0163-4d50-9f99-de781a64cad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=================================================================================\n",
      "\n",
      " Waiting for risk evaluation of MRM monitor 2a3b1d9a-42e4-492f-a09e-084f4f33a306 \n",
      "\n",
      "=================================================================================\n",
      "\n",
      "\n",
      "\n",
      "upload_in_progress.\n",
      "running...\n",
      "finished\n",
      "\n",
      "---------------------------------------\n",
      " Successfully finished evaluating risk \n",
      "---------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_set_name = \"data\"\n",
    "content_type = \"multipart/form-data\"\n",
    "body = {}\n",
    "\n",
    "# Preparing the test data, removing extra columns\n",
    "cols_to_remove = [\"uid\", \"doc\", \"title\", \"id\"]\n",
    "for col in cols_to_remove:\n",
    "    if col in llm_data:\n",
    "        del llm_data[col]\n",
    "llm_data.to_csv(test_data_path, index=False)\n",
    "\n",
    "response  = wos_client.monitor_instances.mrm.evaluate_risk(\n",
    "    monitor_instance_id=mrm_monitor_instance_id,\n",
    "    test_data_set_name=test_data_set_name, \n",
    "    test_data_path=test_data_path,\n",
    "    content_type=content_type,\n",
    "    body=body,\n",
    "    project_id=project_id,\n",
    "    includes_model_output=True,\n",
    "    background_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "6aa46ced-2a61-4c65-8a1e-99a805d3093f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'id': '5ffdaf6b-c29d-467f-b838-6a6552acfe00',\n",
       "  'created_at': '2024-12-31T14:37:57.910Z',\n",
       "  'created_by': 'iam-ServiceId-b317a8da-d926-496e-b0ca-6bcc57f556ae'},\n",
       " 'entity': {'triggered_by': 'user',\n",
       "  'parameters': {'evaluation_start_time': '2024-12-31T14:37:44.064852Z',\n",
       "   'evaluator_user_key': '566e28ff-b469-43ae-8a12-493b549ecb8a',\n",
       "   'facts': {'state': 'finished'},\n",
       "   'is_auto_evaluated': False,\n",
       "   'measurement_id': 'b902db09-33e2-4148-bea4-d20128690557',\n",
       "   'monitors_run_status': [{'monitor_id': 'generative_ai_quality',\n",
       "     'status': {'state': 'finished'}},\n",
       "    {'monitor_id': 'model_health', 'status': {'state': 'finished'}}],\n",
       "   'project_id': 'f1774d00-d497-459c-b31f-74d588bf16e4',\n",
       "   'prompt_template_asset_id': '52c3f9d5-9db3-4b5e-a33c-7947a5e8a576',\n",
       "   'user_iam_id': 'IBMid-550002SR1C',\n",
       "   'wos_created_deployment_id': '1de4eb8d-849d-4772-9b4a-ae23f6937e0a',\n",
       "   'publish_metrics': 'false',\n",
       "   'evaluation_tests': ['drift_v2',\n",
       "    'fairness',\n",
       "    'generative_ai_quality',\n",
       "    'model_health',\n",
       "    'quality']},\n",
       "  'status': {'state': 'finished',\n",
       "   'queued_at': '2024-12-31T14:37:57.897000Z',\n",
       "   'started_at': '2024-12-31T14:37:58.767000Z',\n",
       "   'updated_at': '2024-12-31T14:38:32.075000Z',\n",
       "   'completed_at': '2024-12-31T14:38:27.570000Z',\n",
       "   'message': 'Mrm evaluation complete.',\n",
       "   'operators': []}}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response  = wos_client.monitor_instances.mrm.get_risk_evaluation(mrm_monitor_instance_id, project_id=project_id)\n",
    "response.result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29ac7313-a9c6-4699-8a2a-07bfece896da"
   },
   "source": [
    "### Display the Model Risk metrics.\n",
    "\n",
    "Having calculated the measurements for the Foundation Model subscription, the Model Risk metrics generated for this subscription are available for your review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "082b49fa-6664-4d3d-99cc-0208a9e01d3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<HTML>\n",
       "        <body>\n",
       "            <h3>2a3b1d9a-42e4-492f-a09e-084f4f33a306 Monitor Runs Metrics from: 2024-12-24 14:38:37.549993  till: 2024-12-31 14:38:37.549993</h3>\n",
       "            <table style='border: 1px solid #dddddd; font-family: Courier'>\n",
       "                <th style='border: 1px solid #dddddd'>ts</th><th style='border: 1px solid #dddddd'>id</th><th style='border: 1px solid #dddddd'>measurement_id</th><th style='border: 1px solid #dddddd'>value</th><th style='border: 1px solid #dddddd'>lower_limit</th><th style='border: 1px solid #dddddd'>upper_limit</th><th style='border: 1px solid #dddddd'>tags</th><th style='border: 1px solid #dddddd'>monitor_definition_id</th><th style='border: 1px solid #dddddd'>monitor_instance_id</th><th style='border: 1px solid #dddddd'>run_id</th><th style='border: 1px solid #dddddd'>target_type</th><th style='border: 1px solid #dddddd'>target_id</th>\n",
       "                <tr><td style='border: 1px solid #dddddd'>2024-12-31 14:37:57.976000+00:00</td><td style='border: 1px solid #dddddd'>tests_passed</td><td style='border: 1px solid #dddddd'>b902db09-33e2-4148-bea4-d20128690557</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['test_data_set_name:data']</td><td style='border: 1px solid #dddddd'>mrm</td><td style='border: 1px solid #dddddd'>2a3b1d9a-42e4-492f-a09e-084f4f33a306</td><td style='border: 1px solid #dddddd'>5ffdaf6b-c29d-467f-b838-6a6552acfe00</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:37:57.976000+00:00</td><td style='border: 1px solid #dddddd'>tests_run</td><td style='border: 1px solid #dddddd'>b902db09-33e2-4148-bea4-d20128690557</td><td style='border: 1px solid #dddddd'>1.0</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['test_data_set_name:data']</td><td style='border: 1px solid #dddddd'>mrm</td><td style='border: 1px solid #dddddd'>2a3b1d9a-42e4-492f-a09e-084f4f33a306</td><td style='border: 1px solid #dddddd'>5ffdaf6b-c29d-467f-b838-6a6552acfe00</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:37:57.976000+00:00</td><td style='border: 1px solid #dddddd'>tests_skipped</td><td style='border: 1px solid #dddddd'>b902db09-33e2-4148-bea4-d20128690557</td><td style='border: 1px solid #dddddd'>3.0</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['test_data_set_name:data']</td><td style='border: 1px solid #dddddd'>mrm</td><td style='border: 1px solid #dddddd'>2a3b1d9a-42e4-492f-a09e-084f4f33a306</td><td style='border: 1px solid #dddddd'>5ffdaf6b-c29d-467f-b838-6a6552acfe00</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:37:57.976000+00:00</td><td style='border: 1px solid #dddddd'>tests_failed</td><td style='border: 1px solid #dddddd'>b902db09-33e2-4148-bea4-d20128690557</td><td style='border: 1px solid #dddddd'>1.0</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['test_data_set_name:data']</td><td style='border: 1px solid #dddddd'>mrm</td><td style='border: 1px solid #dddddd'>2a3b1d9a-42e4-492f-a09e-084f4f33a306</td><td style='border: 1px solid #dddddd'>5ffdaf6b-c29d-467f-b838-6a6552acfe00</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr>\n",
       "            </table>\n",
       "        </body>\n",
       "        </HTML>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=mrm_monitor_instance_id, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "42aea40b-2981-4a53-8610-421a5a98cc1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30da4b82-c8f4-43ce-8c75-6b442d7fea1a'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor_definition_id = \"generative_ai_quality\"\n",
    "result = wos_client.monitor_instances.list(\n",
    "    data_mart_id=data_mart_id,\n",
    "    monitor_definition_id=monitor_definition_id,\n",
    "    target_target_id=subscription_id,\n",
    "    project_id=project_id\n",
    ").result\n",
    "result_json = result._to_dict()\n",
    "genaiquality_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "genaiquality_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d402b72-4a53-4d17-a94c-d590a45ecc50"
   },
   "source": [
    "## Display the Generative AI quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "36e57b27-4387-4313-baee-40ebcb90bb1d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<HTML>\n",
       "        <body>\n",
       "            <h3>30da4b82-c8f4-43ce-8c75-6b442d7fea1a Monitor Runs Metrics from: 2024-12-24 14:38:39.595169  till: 2024-12-31 14:38:39.595169</h3>\n",
       "            <table style='border: 1px solid #dddddd; font-family: Courier'>\n",
       "                <th style='border: 1px solid #dddddd'>ts</th><th style='border: 1px solid #dddddd'>id</th><th style='border: 1px solid #dddddd'>measurement_id</th><th style='border: 1px solid #dddddd'>value</th><th style='border: 1px solid #dddddd'>lower_limit</th><th style='border: 1px solid #dddddd'>upper_limit</th><th style='border: 1px solid #dddddd'>tags</th><th style='border: 1px solid #dddddd'>monitor_definition_id</th><th style='border: 1px solid #dddddd'>monitor_instance_id</th><th style='border: 1px solid #dddddd'>run_id</th><th style='border: 1px solid #dddddd'>target_type</th><th style='border: 1px solid #dddddd'>target_id</th>\n",
       "                <tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>hap_input_score</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>rouge2</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>1.0</td><td style='border: 1px solid #dddddd'>0.8</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>faithfulness</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>0.65</td><td style='border: 1px solid #dddddd'>0.7</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>average_precision</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>0.7</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>records_processed</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>4.0</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>hit_rate</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>0.7</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>rougelsum</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>1.0</td><td style='border: 1px solid #dddddd'>0.8</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>answer_relevance</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>1.0</td><td style='border: 1px solid #dddddd'>0.7</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>hap_score</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr><tr><td style='border: 1px solid #dddddd'>2024-12-31 14:38:16.896349+00:00</td><td style='border: 1px solid #dddddd'>reciprocal_rank</td><td style='border: 1px solid #dddddd'>f28e7dc9-b3e2-4812-b7bd-4c5ad5c62046</td><td style='border: 1px solid #dddddd'>0.0</td><td style='border: 1px solid #dddddd'>0.7</td><td style='border: 1px solid #dddddd'>None</td><td style='border: 1px solid #dddddd'>['computed_on:feedback', 'field_type:subscription', 'aggregation_type:mean']</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>30da4b82-c8f4-43ce-8c75-6b442d7fea1a</td><td style='border: 1px solid #dddddd'>ac4a4705-827f-475d-b6de-e9a1b55a8eed</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>57221ce7-a6a0-4b80-a08c-0268c56631cd</td></tr>\n",
       "            </table>\n",
       "        </body>\n",
       "        </HTML>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: First 10 records were displayed.\n"
     ]
    }
   ],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=genaiquality_monitor_id, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32dc6e4e-af89-4130-a24e-61b450769379"
   },
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have completed this notebook. You can now navigate to the prompt template asset in your watsonx.governance project / space and click on the `Evaluate` tab to visualize the results in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a26c2e9f-0696-4576-a0bf-bcd91084b634"
   },
   "source": [
    "Author: ravi.chamarthy@in.ibm.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
